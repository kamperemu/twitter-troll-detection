{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "knowing-meditation",
   "metadata": {},
   "source": [
    "# Twitter Troll Detection (sklearn algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-navigation",
   "metadata": {},
   "source": [
    "The full version of the project is available on https://github.com/kamperemu/twitter-troll-detection\n",
    "\n",
    "Now we will install the modules used in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn==0.11.1\n",
    "!pip install matplotlib==3.3.2\n",
    "!pip install nltk==3.5\n",
    "!pip install sklearn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-instrumentation",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-retro",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "The dataset is shuffled and the split into xtrain, ytrain, xtest and ytest. The output of the cell shows first five of the training text and its corresponding labels and the first five of the testing text and its corresponidng labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/test.json\", 'r') as f:\n",
    "    tweets = json.load(f)\n",
    "random.shuffle(tweets)\n",
    "train = tweets[:int(round(4*len(tweets)/5))]\n",
    "test = tweets[int(round(4*len(tweets)/5)):len(tweets)]\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "xtest = []\n",
    "ytest = []\n",
    "\n",
    "for tweet in train:\n",
    "    xtrain.append(tweet['content'])\n",
    "    ytrain.append(tweet['label'])\n",
    "    \n",
    "for tweet in test:\n",
    "    xtest.append(tweet['content'])\n",
    "    ytest.append(tweet['label'])\n",
    "\n",
    "    \n",
    "print(xtrain[:5])\n",
    "print(ytrain[:5])\n",
    "print(xtest[:5])\n",
    "print(ytest[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-nurse",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "The special characters in text data are removed and made lowercase. Then all the words are converted to the root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessklearn.py file (you are actually supposed to import this but for demonstration i have just put this in)\n",
    "# you can do this by putting the line \"from preprocesssklearn import *\"\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def removespchar(text):\n",
    "    pattern=r'[^a-zA-Z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text.lower()\n",
    "\n",
    "def stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-flavor",
   "metadata": {},
   "source": [
    "The output shows the pre-processed data of the first five training text and first five testing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(xtrain)):\n",
    "    xtrain[i] = removespchar(xtrain[i])\n",
    "    xtrain[i] = stemmer(xtrain[i])\n",
    "for i in range(len(xtest)):\n",
    "    xtest[i] = removespchar(xtest[i])\n",
    "    xtest[i] = stemmer(xtest[i])\n",
    "    \n",
    "print(xtrain[:5])\n",
    "print(xtest[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-delaware",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "Created BoW and Tfidf encoded arrays from the pre-processed text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "cvxtrain=cv.fit_transform(xtrain)\n",
    "cvxtest=cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "tvxtrain=tv.fit_transform(xtrain)\n",
    "tvxtest=tv.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-terrorist",
   "metadata": {},
   "source": [
    "## Classifier - training and testing\n",
    "We train and test the Support Vector Machine, Naive Bayes and Logistic Regression algorithms first with the BoW vectorizer and then with the Tfidf Vectorizer. Each cell gives the output of the predicted values of the first five testing data followed by the acutal value of the testing data. Finally each cell also has an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvsvm=LinearSVC()\n",
    "cvsvm.fit(cvxtrain,ytrain)\n",
    "cvsvmpred = cvsvm.predict(cvxtest)\n",
    "print(cvsvmpred[:5])\n",
    "print(ytest[:5])\n",
    "print(\"Support Vector Machine Accuracy Score -> \",accuracy_score(cvsvmpred, ytest)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvnb=naive_bayes.MultinomialNB()\n",
    "cvnb.fit(cvxtrain,ytrain)\n",
    "cvnbpred = cvnb.predict(cvxtest)\n",
    "print(cvnbpred[:5])\n",
    "print(ytest[:5])\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(cvnbpred, ytest)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvlr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "cvlr.fit(cvxtrain,ytrain)\n",
    "cvlrpred = cvlr.predict(cvxtest)\n",
    "print(cvlrpred[:5])\n",
    "print(ytest[:5])\n",
    "print(\"Logistic Regression Accuracy Score -> \",accuracy_score(cvlrpred, ytest)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsvm=LinearSVC()\n",
    "tvsvm.fit(tvxtrain,ytrain)\n",
    "tvsvmpred = tvsvm.predict(tvxtest)\n",
    "print(tvsvmpred[:5])\n",
    "print(ytest[:5])\n",
    "print(\"Support Vector Machine Accuracy Score -> \",accuracy_score(tvsvmpred, ytest)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvnb=naive_bayes.MultinomialNB()\n",
    "tvnb.fit(tvxtrain,ytrain)\n",
    "tvnbpred = tvnb.predict(tvxtest)\n",
    "print(tvnbpred[:5])\n",
    "print(ytest[:5])\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(tvnbpred, ytest)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvlr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "tvlr.fit(tvxtrain,ytrain)\n",
    "tvlrpred = tvlr.predict(tvxtest)\n",
    "print(tvlrpred[:5])\n",
    "print(ytest[:5])\n",
    "print(\"Logistic Regression Accuracy Score -> \",accuracy_score(tvlrpred, ytest)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-lloyd",
   "metadata": {},
   "source": [
    "Confusion matrix of each of the algorithms is outputed in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(ytest,cvsvmpred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(ytest,cvnbpred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(ytest,cvlrpred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(ytest,tvsvmpred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(ytest,tvnbpred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(ytest,tvlrpred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-gilbert",
   "metadata": {},
   "source": [
    "## Saving the models\n",
    "We dump all the variable used for the algorithm for further use using the pickle dump function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv, open(\"savedModel/sklearnpynb/BoW.sav\",\"wb\"))\n",
    "pickle.dump(cvnb, open(\"savedModel/sklearnpynb/cvnb.sav\",\"wb\"))\n",
    "pickle.dump(cvsvm, open(\"savedModel/sklearnpynb/cvsvm.sav\",\"wb\"))\n",
    "pickle.dump(cvlr, open(\"savedModel/sklearnpynb/cvlr.sav\",\"wb\"))\n",
    "pickle.dump(tv, open(\"savedModel/sklearnpynb/Tfidf.sav\",\"wb\"))\n",
    "pickle.dump(tvnb, open(\"savedModel/sklearnpynb/tvnb.sav\",\"wb\"))\n",
    "pickle.dump(tvsvm, open(\"savedModel/sklearnpynb/tvsvm.sav\",\"wb\"))\n",
    "pickle.dump(tvlr, open(\"savedModel/sklearnpynb/tvlr.sav\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-somalia",
   "metadata": {},
   "source": [
    "The code below loads the previously saved variables and uses and classifies new text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# supposed to put \"from preprocesssklearn import *\" from cell 4\n",
    "n = int(input(\"no of sentences: \"))\n",
    "sentences = [str(input(\"enter sentence:\")) for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = removespchar(sentences[i])\n",
    "    sentences[i] = stemmer(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pickle.load(open(\"savedModel/sklearnpynb/BoW.sav\",\"rb\"))\n",
    "cvnb = pickle.load(open(\"savedModel/sklearnpynb/cvnb.sav\",\"rb\"))\n",
    "cvsvm = pickle.load(open(\"savedModel/sklearnpynb/cvsvm.sav\",\"rb\"))\n",
    "cvlr = pickle.load(open(\"savedModel/sklearnpynb/cvlr.sav\",\"rb\"))\n",
    "tv = pickle.load(open(\"savedModel/sklearnpynb/Tfidf.sav\",\"rb\"))\n",
    "tvnb = pickle.load(open(\"savedModel/sklearnpynb/tvnb.sav\",\"rb\"))\n",
    "tvsvm = pickle.load(open(\"savedModel/sklearnpynb/tvsvm.sav\",\"rb\"))\n",
    "tvlr = pickle.load(open(\"savedModel/sklearnpynb/tvlr.sav\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvsentences = cv.transform(sentences)\n",
    "print(cvsvm.predict(cvsentences))\n",
    "print(cvnb.predict(cvsentences))\n",
    "print(cvlr.predict(cvsentences))\n",
    "tvsentences = tv.transform(sentences)\n",
    "print(tvsvm.predict(cvsentences))\n",
    "print(tvnb.predict(cvsentences))\n",
    "print(tvlr.predict(cvsentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
