{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/smallerdata.json\", 'r') as f:\n",
    "    tweets = json.load(f)\n",
    "random.shuffle(tweets)\n",
    "train = tweets[:int(round(4*len(tweets)/5))]\n",
    "test = tweets[int(round(4*len(tweets)/5)):len(tweets)]\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "xtest = []\n",
    "ytest = []\n",
    "\n",
    "for tweet in train:\n",
    "    xtrain.append(tweet['content'])\n",
    "    ytrain.append(tweet['label'])\n",
    "    \n",
    "for tweet in test:\n",
    "    xtest.append(tweet['content'])\n",
    "    ytest.append(tweet['label'])\n",
    "\n",
    "    \n",
    "print(xtrain[:5])\n",
    "print(ytrain[:5])\n",
    "print(xtest[:5])\n",
    "print(ytest[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessklearn.py file (you are actually supposed to import this but for demonstration i have just put this in)\n",
    "# you can do this by putting the line \"from preprocesssklearn import *\"\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "def removespchar(text):\n",
    "    pattern=r'[^a-zA-Z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "def stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def removestopwords(text, islowercase=True):\n",
    "    tokenizer=ToktokTokenizer()\n",
    "    stopwordList=nltk.corpus.stopwords.words('english')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if islowercase:\n",
    "        filteredTokens = [token for token in tokens if token not in stopwordList]\n",
    "    else:\n",
    "        filteredTokens = [token for token in tokens if token.lower() not in stopwordList]\n",
    "    filteredText = ' '.join(filteredTokens)    \n",
    "    return filteredText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(xtrain)):\n",
    "    xtrain[i] = removespchar(xtrain[i])\n",
    "    xtrain[i] = stemmer(xtrain[i])\n",
    "    xtrain[i] = removestopwords(xtrain[i])\n",
    "\n",
    "for i in range(len(xtest)):\n",
    "    xtest[i] = removespchar(xtest[i])\n",
    "    xtest[i] = stemmer(xtest[i])\n",
    "    xtest[i] = removestopwords(xtest[i])\n",
    "    \n",
    "print(xtrain[:5])\n",
    "print(xtest[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "xtrain=tv.fit_transform(xtrain)\n",
    "xtest=tv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=LinearSVC()\n",
    "svm.fit(xtrain,ytrain)\n",
    "svmpred = svm.predict(xtest)\n",
    "print(svmpred[:5])\n",
    "print(ytest[:5])\n",
    "print(\"Support Vector Machine Accuracy Score -> \",accuracy_score(svmpred, ytest)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(ytest,svmpred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=naive_bayes.MultinomialNB()\n",
    "nb.fit(xtrain,ytrain)\n",
    "nbpred = nb.predict(xtest)\n",
    "print(nbpred[:5])\n",
    "print(ytest[:5])\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(nbpred, ytest)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(ytest,nbpred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "lr.fit(xtrain,ytrain)\n",
    "lrpred = lr.predict(xtest)\n",
    "print(lrpred[:5])\n",
    "print(ytest[:5])\n",
    "print(\"Logistic Regression Accuracy Score -> \",accuracy_score(lrpred, ytest)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = confusion_matrix(ytest,lrpred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from preprocesssklearn import *\n",
    "pickle.dump(cv, open(\"savedModel/sklearnpynb/BoW.sav\",\"wb\"))\n",
    "pickle.dump(nb, open(\"savedModel/sklearnpynb/nb.sav\",\"wb\"))\n",
    "pickle.dump(svm, open(\"savedModel/sklearnpynb/svm.sav\",\"wb\"))\n",
    "pickle.dump(lr, open(\"savedModel/sklearnpynb/lr.sav\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(input(\"no of sentences: \"))\n",
    "sentences = [str(input(\"enter sentence:\")) for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = removespchar(sentences[i])\n",
    "    sentences[i] = stemmer(sentences[i])\n",
    "    sentences[i] = removestopwords(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pickle.load(open(\"savedModel/sklearnpynb/BoW.sav\",\"rb\"))\n",
    "nb = pickle.load(open(\"savedModel/sklearnpynb/nb.sav\",\"rb\"))\n",
    "svm = pickle.load(open(\"savedModel/sklearnpynb/svm.sav\",\"rb\"))\n",
    "lr = pickle.load(open(\"savedModel/sklearnpynb/lr.sav\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = cv.transform(sentences)\n",
    "print(svm.predict(sentences))\n",
    "print(nb.predict(sentences))\n",
    "print(lr.predict(sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
