{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cardiovascular-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes unnecessary logs\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# imports required for the training algorithm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unnecessary-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined variables\n",
    "vocabSize = 10000\n",
    "outputDim = 16\n",
    "maxInput = 200\n",
    "truncType='post'\n",
    "padType='post'\n",
    "oov = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cordless-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/test.json\", 'r') as f:\n",
    "    tweets = json.load(f)\n",
    "random.shuffle(tweets)\n",
    "train = tweets[:int(round(4*len(tweets)/5))]\n",
    "test = tweets[int(round(4*len(tweets)/5)):len(tweets)]\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "xtest = []\n",
    "ytest = []\n",
    "\n",
    "for tweet in train:\n",
    "    xtrain.append(tweet['content'])\n",
    "    ytrain.append(tweet['label'])\n",
    "    \n",
    "for tweet in test:\n",
    "    xtest.append(tweet['content'])\n",
    "    ytest.append(tweet['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "departmental-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization go brrr\n",
    "tokenizer = Tokenizer(num_words=vocabSize, oov_token=oov)\n",
    "tokenizer.fit_on_texts(xtrain)\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "\n",
    "# preparing training data for neural network\n",
    "xtrainencoded = tokenizer.texts_to_sequences(xtrain)\n",
    "xtrainpadded = pad_sequences(xtrainencoded, maxlen=maxInput, padding=padType, truncating=truncType)\n",
    "xtrain = np.asarray(xtrainpadded).astype(np.float32)\n",
    "ytrain = np.asarray(ytrain).astype(np.float32)\n",
    "\n",
    "# preparing testing data for neural network\n",
    "xtestencoded = tokenizer.texts_to_sequences(xtest)\n",
    "xtestpadded = pad_sequences(xtestencoded, maxlen=maxInput, padding=padType, truncating=truncType)\n",
    "xtest = np.asarray(xtestpadded).astype(np.float32)\n",
    "ytest = np.asarray(ytest).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "actual-smart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the model\n",
      "Epoch 1/20\n",
      "750/750 - 8s - loss: 0.4683 - accuracy: 0.8378 - val_loss: 0.2432 - val_accuracy: 0.9172\n",
      "Epoch 2/20\n",
      "750/750 - 3s - loss: 0.1705 - accuracy: 0.9435 - val_loss: 0.1382 - val_accuracy: 0.9527\n",
      "Epoch 3/20\n",
      "750/750 - 3s - loss: 0.1071 - accuracy: 0.9641 - val_loss: 0.1056 - val_accuracy: 0.9647\n",
      "Epoch 4/20\n",
      "750/750 - 3s - loss: 0.0793 - accuracy: 0.9737 - val_loss: 0.0941 - val_accuracy: 0.9713\n",
      "Epoch 5/20\n",
      "750/750 - 3s - loss: 0.0647 - accuracy: 0.9793 - val_loss: 0.0836 - val_accuracy: 0.9720\n",
      "Epoch 6/20\n",
      "750/750 - 3s - loss: 0.0549 - accuracy: 0.9818 - val_loss: 0.0798 - val_accuracy: 0.9732\n",
      "Epoch 7/20\n",
      "750/750 - 3s - loss: 0.0466 - accuracy: 0.9850 - val_loss: 0.0783 - val_accuracy: 0.9735\n",
      "Epoch 8/20\n",
      "750/750 - 4s - loss: 0.0403 - accuracy: 0.9873 - val_loss: 0.0786 - val_accuracy: 0.9743\n",
      "Epoch 9/20\n",
      "750/750 - 3s - loss: 0.0360 - accuracy: 0.9882 - val_loss: 0.0806 - val_accuracy: 0.9737\n",
      "Epoch 10/20\n",
      "750/750 - 3s - loss: 0.0317 - accuracy: 0.9900 - val_loss: 0.0819 - val_accuracy: 0.9748\n",
      "Epoch 11/20\n",
      "750/750 - 3s - loss: 0.0286 - accuracy: 0.9911 - val_loss: 0.0831 - val_accuracy: 0.9743\n",
      "Epoch 12/20\n",
      "750/750 - 3s - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.0871 - val_accuracy: 0.9728\n",
      "Epoch 13/20\n",
      "750/750 - 4s - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0888 - val_accuracy: 0.9735\n",
      "Epoch 14/20\n",
      "750/750 - 4s - loss: 0.0221 - accuracy: 0.9931 - val_loss: 0.0942 - val_accuracy: 0.9733\n",
      "Epoch 15/20\n",
      "750/750 - 4s - loss: 0.0205 - accuracy: 0.9936 - val_loss: 0.0961 - val_accuracy: 0.9728\n",
      "Epoch 16/20\n",
      "750/750 - 4s - loss: 0.0192 - accuracy: 0.9937 - val_loss: 0.1001 - val_accuracy: 0.9708\n",
      "Epoch 17/20\n",
      "750/750 - 4s - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.1044 - val_accuracy: 0.9702\n",
      "Epoch 18/20\n",
      "750/750 - 3s - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.1117 - val_accuracy: 0.9720\n",
      "Epoch 19/20\n",
      "750/750 - 3s - loss: 0.0157 - accuracy: 0.9953 - val_loss: 0.1124 - val_accuracy: 0.9688\n",
      "Epoch 20/20\n",
      "750/750 - 3s - loss: 0.0150 - accuracy: 0.9950 - val_loss: 0.1187 - val_accuracy: 0.9717\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocabSize, outputDim, input_length=maxInput),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "numEpochs = 10\n",
    "print(\"training the model\")\n",
    "history = model.fit(xtrain, ytrain, epochs=numEpochs, validation_data=(xtest, ytest), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "plt.figure()\n",
    "pred = tf.round(model.predict(xtestpadded))\n",
    "array = confusion_matrix(tf.round(ytest),pred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "restricted-stick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: savedModel/nn/basic/model\\assets\n"
     ]
    }
   ],
   "source": [
    "# saving the tokenizer\n",
    "with open('savedModel/nn/basic/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# saving trained model\n",
    "model.save(\"savedModel/nn/basic/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes unnecessary logs\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# predefined variables\n",
    "maxInput = 200\n",
    "truncType='post'\n",
    "padType='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer and model\n",
    "with open('savedModel/nn/basic/tokenizer.pickle', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "\n",
    "model = tf.keras.models.load_model(\"savedModel/nn/basic/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(input(\"no of sentences: \"))\n",
    "sentences = [input(\"Enter sentence:\") for _ in range(n)]\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, maxlen=maxInput, padding=padType, truncating=truncType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in model.predict(padded):\n",
    "    for j in i:\n",
    "        pred.append(int(j))\n",
    "\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
