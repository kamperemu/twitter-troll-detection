{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes unnecessary logs\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# imports required for the training algorithm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined variables\n",
    "vocabSize = 10000\n",
    "outputDim = 16\n",
    "maxInput = 200\n",
    "truncType='post'\n",
    "padType='post'\n",
    "oov = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/smallerdata.json\", 'r') as f:\n",
    "    tweets = json.load(f)\n",
    "random.shuffle(tweets)\n",
    "train = tweets[:int(round(4*len(tweets)/5))]\n",
    "test = tweets[int(round(4*len(tweets)/5)):len(tweets)]\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "xtest = []\n",
    "ytest = []\n",
    "\n",
    "for tweet in train:\n",
    "    xtrain.append(tweet['content'])\n",
    "    ytrain.append(tweet['label'])\n",
    "    \n",
    "for tweet in test:\n",
    "    xtest.append(tweet['content'])\n",
    "    ytest.append(tweet['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization go brrr\n",
    "tokenizer = Tokenizer(num_words=vocabSize, oov_token=oov)\n",
    "tokenizer.fit_on_texts(xtrain)\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "\n",
    "# preparing training data for neural network\n",
    "xtrainencoded = tokenizer.texts_to_sequences(xtrain)\n",
    "xtrainpadded = pad_sequences(xtrainencoded, maxlen=maxInput, padding=padType, truncating=truncType)\n",
    "xtrain = np.asarray(xtrainpadded).astype(np.float32)\n",
    "ytrain = np.asarray(ytrain).astype(np.float32)\n",
    "\n",
    "# preparing testing data for neural network\n",
    "xtestencoded = tokenizer.texts_to_sequences(xtest)\n",
    "xtestpadded = pad_sequences(xtestencoded, maxlen=maxInput, padding=padType, truncating=truncType)\n",
    "xtest = np.asarray(xtestpadded).astype(np.float32)\n",
    "ytest = np.asarray(ytest).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocabSize, outputDim, input_length=maxInput),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "numEpochs = 20\n",
    "print(\"training the model\")\n",
    "history = model.fit(xtrain, ytrain, epochs=numEpochs, validation_data=(xtest, ytest), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "plt.figure()\n",
    "pred = tf.round(model.predict(xtestpadded))\n",
    "array = confusion_matrix(tf.round(ytest),pred,labels=[1,0])\n",
    "df_cm = pd.DataFrame(array, range(2), range(2))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tokenizer\n",
    "with open('savedModel/nn/basic/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# saving trained model\n",
    "model.save(\"savedModel/nn/basic/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes unnecessary logs\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# predefined variables\n",
    "maxInput = 200\n",
    "truncType='post'\n",
    "padType='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tokenizer and model\n",
    "with open('savedModel/nn/basic/tokenizer.pickle', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "\n",
    "model = tf.keras.models.load_model(\"savedModel/nn/basic/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(input(\"no of sentences: \"))\n",
    "sentences = [input(\"Enter sentence:\") for _ in range(n)]\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, maxlen=maxInput, padding=padType, truncating=truncType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in model.predict(padded):\n",
    "    for j in i:\n",
    "        pred.append(int(j))\n",
    "\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
